# ! usr/bin/env python

import time

import gym
import numpy as np
from gym import spaces
from gym.utils import seeding

import rospy
from gazebo_msgs.srv import GetModelState
from ros_interface import OpenManipulatorRosInterface

# Global variables
ACTION_DIM = 3  # Cartesian
OBSERVATION_DIM = 4
X_MIN = 0.1
X_MAX = 0.5
Y_MIN = -0.3
Y_MAX = 0.3
Z_MIN = 0.0
Z_MAX = 0.6
TERM_COUNT = 10
SUC_COUNT = 10


class OpenManipulatorReacherEnv(gym.Env):
    def __init__(
        self,
        max_steps=700,
        is_dagger=False,
        is_pomdp=False,
        is_real=False,
        train_indicator=0,
    ):

        action_space = spaces.Box(-1.0, 1.0, shape=(ACTION_DIM,), dtype="float32")
        self.action_space = action_space

        observation_space = spaces.Box(
            -np.inf, np.inf, shape=(OBSERVATION_DIM,), dtype="float32"
        )
        observation_space_dict = dict(observation=observation_space)
        self.observation_space = spaces.Dict(observation_space_dict)

        self.ros_interface = OpenManipulatorRosInterface()
        self.train_indicator = train_indicator
        self.is_dagger = is_dagger
        self.is_pomdp = is_pomdp
        self.is_real = is_real
        self.max_steps = max_steps
        self.done = False
        self.reward = 0
        self.reward_rescale = 1.0
        self.reward_type = "sparse"
        self.termination_count = 0
        self.success_count = 0
        self.step_cnt = 0
        self.tic = 0.0
        self.toc = 0.0
        self.elapsed = 0.0

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        """Function executed each time step.
        Here we get the action execute it in a time step and retrieve the
        observations generated by that action.

        TODO: google stype docstrings
        :param action:
        :return: obs, reward, done
        """
        if action is None:
            action = np.array([1, 1, 1, 1, 1, 1])

        self.prev_tic = self.tic
        self.tic = time.time()
        self.elapsed = time.time() - self.prev_tic
        self.done = False

        if self.step_cnt == self.max_steps:
            self.done = True
            self.step_cnt = 0

        act = action.flatten().tolist()
        self.ros_interface.set_joints_position(act)
        curDist = self._get_dist()

        if not self.is_real:
            self.reward = self._compute_reward()
            if self._check_for_termination():
                print("======================================================")
                print("Terminates current Episode : OUT OF BOUNDARY")
                print("======================================================")
            elif self._check_for_success():
                print("======================================================")
                print("Succeeded current Episode")
                print("======================================================")
        _joint_pos, _joint_vels, _joint_effos = self.ros_interface.get_joints_states()
        # obj_pos = self._get_target_obj_obs() # TODO: implement this
        #  function call.

        if np.mod(self.step_cnt, 10) == 0:
            if not self.is_real:
                print("DISTANCE : ", curDist)
            print("PER STEP ELAPSED : ", self.elapsed)
            print("SPARSE REWARD : ", self.reward_rescale * self.reward)
            print("Current EE pos: ", self.ros_interface.gripper_position)
            print("Actions: ", act)

        obs = np.array([_joint_pos, _joint_vels, _joint_effos])
        self.step_cnt += 1

        return obs, self.reward_rescale * self.reward, self.done

    def reset(self):
        """Attempt to reset the simulator.

        Since we randomize initial conditions, it is possible to get into
        a state with numerical issues (e.g. due to penetration or
        Gimbel lock) or we may not achieve an initial condition (e.g. an
        object is within the hand).

        In this case, we just keep randomizing until we eventually achieve
        a valid initial
        configuration.
        """
        # TODO: who are you?
        # did_reset_sim = False
        self.ros_interface._reset_gazebo_world()
        _joint_pos, _joint_vels, _joint_effos = self.ros_interface.get_joints_states()
        obs = np.array([_joint_pos, _joint_vels, _joint_effos])

        return obs

    def close(self):
        # TODO: Docstrings
        rospy.signal_shutdown("done")

    def _check_robot_moving(self):
        """Check if robot has reached its initial pose."""
        while not rospy.is_shutdown():
            if self.moving_state == "STOPPED":
                break
        return True

    def _check_for_success(self):
        """Check if the agent has succeeded the episode."""
        _dist = self._get_dist()
        if _dist < self.ros_interface.distance_threshold:
            self.success_count += 1
            if self.success_count == SUC_COUNT:
                self.done = True
                self.success_count = 0
                return True
            # TODO: return what in else statement??
        else:
            return False

    def _check_for_termination(self):
        """Check if the agent has reached undesirable state.

        If so, terminate the episode early.
        """
        _ee_pose = self.ros_interface.get_gripper_position()
        if not (
            (X_MIN < _ee_pose[0] < X_MAX)
            and (Y_MIN < _ee_pose[1] < Y_MAX)
            and (Z_MIN < _ee_pose[1] < Z_MAX)
        ):
            self.termination_count += 1

        if self.termination_count == TERM_COUNT:
            self.done = True
            self.termination_count = 0
            return True
        else:
            return False

    def _compute_reward(self):
        """Computes shaped/sparse reward for each episode."""
        cur_dist = self._get_dist()
        if self.reward_type == "sparse":
            # 1 for success else 0
            reward = cur_dist <= self.ros_interface.distance_threshold
            reward = reward.astype(np.float32)
            return reward
        else:
            # -L2 distance
            reward = -cur_dist - self.squared_sum_vel
            return reward

    def _get_dist(self):
        # TODO: docstrings
        rospy.wait_for_service("/gazebo/get_model_state")

        try:
            object_state_srv = rospy.ServiceProxy(
                "/gazebo/get_model_state", GetModelState
            )
            object_state = object_state_srv("block", "world")
            object_pose = [
                object_state.pose.position.x,
                object_state.pose.position.y,
                object_state.pose.position.z,
            ]
            self._obj_pose = np.array(object_pose)
        except rospy.ServiceException as e:
            rospy.logerr("Spawn URDF service call failed: {0}".format(e))

        # FK state of robot
        end_effector_pose = np.array(self.ros_interface.get_gripper_position())

        return np.linalg.norm(end_effector_pose - self._obj_pose)
